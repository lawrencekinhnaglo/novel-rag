"""LLM Service supporting LM Studio and DeepSeek API."""
import httpx
from typing import List, Dict, Any, Optional, AsyncGenerator
from openai import AsyncOpenAI
from app.config import settings
import logging

logger = logging.getLogger(__name__)


class LLMProvider:
    """Base LLM provider interface."""
    
    async def generate(self, messages: List[Dict[str, str]], 
                      temperature: float = 0.7,
                      max_tokens: int = 4096) -> str:
        """Generate a response from the LLM."""
        raise NotImplementedError
    
    async def stream(self, messages: List[Dict[str, str]],
                    temperature: float = 0.7,
                    max_tokens: int = 4096) -> AsyncGenerator[str, None]:
        """Stream a response from the LLM."""
        raise NotImplementedError


class LMStudioProvider(LLMProvider):
    """LM Studio local LLM provider."""
    
    def __init__(self):
        self.client = AsyncOpenAI(
            base_url=settings.LM_STUDIO_URL,
            api_key="lm-studio"  # LM Studio doesn't require a real API key
        )
        self.model = settings.LM_STUDIO_MODEL
    
    async def generate(self, messages: List[Dict[str, str]], 
                      temperature: float = 0.7,
                      max_tokens: int = 4096) -> str:
        """Generate a response from LM Studio."""
        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"LM Studio error: {e}")
            raise
    
    async def stream(self, messages: List[Dict[str, str]],
                    temperature: float = 0.7,
                    max_tokens: int = 4096) -> AsyncGenerator[str, None]:
        """Stream a response from LM Studio."""
        try:
            stream = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=True
            )
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
        except Exception as e:
            logger.error(f"LM Studio streaming error: {e}")
            raise


class DeepSeekProvider(LLMProvider):
    """DeepSeek API provider."""
    
    def __init__(self):
        if not settings.DEEPSEEK_API_KEY:
            raise ValueError("DEEPSEEK_API_KEY not configured")
        self.client = AsyncOpenAI(
            base_url=settings.DEEPSEEK_API_URL,
            api_key=settings.DEEPSEEK_API_KEY
        )
        self.model = settings.DEEPSEEK_MODEL
    
    async def generate(self, messages: List[Dict[str, str]], 
                      temperature: float = 0.7,
                      max_tokens: int = 4096) -> str:
        """Generate a response from DeepSeek."""
        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"DeepSeek error: {e}")
            raise
    
    async def stream(self, messages: List[Dict[str, str]],
                    temperature: float = 0.7,
                    max_tokens: int = 4096) -> AsyncGenerator[str, None]:
        """Stream a response from DeepSeek."""
        try:
            stream = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=True
            )
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
        except Exception as e:
            logger.error(f"DeepSeek streaming error: {e}")
            raise


class LLMService:
    """Unified LLM service that can switch between providers."""
    
    def __init__(self, provider: str = None):
        self.provider_name = provider or settings.DEFAULT_LLM_PROVIDER
        self._provider: Optional[LLMProvider] = None
    
    @property
    def provider(self) -> LLMProvider:
        """Get or create the LLM provider."""
        if self._provider is None:
            self._provider = self._create_provider()
        return self._provider
    
    def _create_provider(self) -> LLMProvider:
        """Create the appropriate LLM provider."""
        if self.provider_name == "lm_studio":
            return LMStudioProvider()
        elif self.provider_name == "deepseek":
            return DeepSeekProvider()
        else:
            raise ValueError(f"Unknown LLM provider: {self.provider_name}")
    
    def switch_provider(self, provider: str):
        """Switch to a different LLM provider."""
        self.provider_name = provider
        self._provider = None
    
    async def generate(self, messages: List[Dict[str, str]], 
                      temperature: float = 0.7,
                      max_tokens: int = 4096) -> str:
        """Generate a response."""
        return await self.provider.generate(messages, temperature, max_tokens)
    
    async def stream(self, messages: List[Dict[str, str]],
                    temperature: float = 0.7,
                    max_tokens: int = 4096) -> AsyncGenerator[str, None]:
        """Stream a response."""
        async for chunk in self.provider.stream(messages, temperature, max_tokens):
            yield chunk
    
    async def generate_with_context(self, user_message: str,
                                    context: Dict[str, Any],
                                    system_prompt: str = None,
                                    conversation_history: List[Dict[str, str]] = None,
                                    temperature: float = 0.7,
                                    language: str = "en",
                                    max_context_tokens: int = 32000,
                                    categories: List[str] = None,
                                    uploaded_content: str = None) -> str:
        """
        Generate a response with RAG context, full novel awareness, and long context support.
        
        Args:
            user_message: The user's question/request
            context: Retrieved context from RAG
            system_prompt: Optional custom system prompt
            conversation_history: Previous conversation messages
            temperature: LLM temperature setting
            language: Response language (en, zh-TW, zh-CN)
            max_context_tokens: Maximum tokens for context
            categories: Knowledge categories to prioritize
        """
        # Build system prompt with context
        if system_prompt is None:
            system_prompt = self._build_novel_system_prompt(language)
        
        # Add context to system prompt
        if context:
            context_text = self._format_context(context, language, categories)
            system_prompt += f"\n\n## Retrieved Context:\n{context_text}"
        
        # Add uploaded content if provided
        if uploaded_content:
            system_prompt += f"\n\n## Uploaded Document Content:\n{uploaded_content[:10000]}"
        
        # Build messages
        messages = [{"role": "system", "content": system_prompt}]
        
        # Add conversation history (more for long context support)
        if conversation_history:
            # For long context models, include more history
            history_limit = 30 if max_context_tokens >= 32000 else 10
            messages.extend(conversation_history[-history_limit:])
        
        # Add user message
        messages.append({"role": "user", "content": user_message})
        
        return await self.generate(messages, temperature)
    
    def _build_novel_system_prompt(self, language: str = "en") -> str:
        """Build the default system prompt for novel writing with full character awareness."""
        
        prompts = {
            "en": """You are an expert novel writing assistant with deep knowledge of storytelling, character development, plot structure, and creative writing.

## Your Role - Full Story Immersion
You are FULLY IMMERSED in this novel's world. You understand and remember:
- Each character's complete personality, motivations, fears, desires, and speech patterns
- How each character would naturally react in any situation based on their established traits
- The complete timeline of events and how past events influence the present
- All world-building details: settings, rules, magic systems, technology, culture
- Ongoing plot threads, conflicts, subplots, and narrative arcs
- Relationships between all characters and how they evolve

## Character Behavior Awareness
When discussing or writing about characters, you MUST:
- Stay TRUE to their established personality - never break character
- Use their characteristic speech patterns, vocabulary, and mannerisms
- Consider their emotional state based on recent story events
- Account for their relationships and history with other characters
- Remember what they know vs. don't know (no character omniscience)
- Reflect their growth/changes if they've developed through the story

## Your Capabilities
- Discussing plot ideas, character arcs, and story structure
- Writing scenes and dialogue that authentically match character voices
- Predicting how characters would react to hypothetical situations
- Identifying plot holes, inconsistencies, or out-of-character moments
- Suggesting plot developments that fit character motivations
- Helping maintain consistency across the entire novel

## Knowledge Categories You Can Access
Your context includes information categorized as:
- **Draft**: Work in progress content
- **Concept**: High-level story ideas and themes
- **Character**: Detailed character profiles and development
- **Chapter**: Published/finalized chapter content
- **Settings**: World-building, magic systems, technology
- **Plot**: Story outlines, arcs, and conflict structures
- **Dialogue**: Sample dialogues and character voice references
- **Research**: Background research and references
- **Notes**: Miscellaneous notes and ideas

## Guidelines
1. ALWAYS maintain consistency with established story elements
2. When unsure, acknowledge it and suggest checking source material
3. Point out potential inconsistencies proactively
4. Respect the author's creative vision while offering suggestions
5. Consider cultural and linguistic nuances in the story world""",

            "zh-TW": """ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„å°èªªå¯«ä½œåŠ©æ‰‹ï¼Œç²¾é€šæ•…äº‹æ•˜è¿°ã€è§’è‰²ç™¼å±•ã€æƒ…ç¯€çµæ§‹å’Œå‰µæ„å¯«ä½œã€‚

## ä½ çš„è§’è‰² - å®Œå…¨æ²‰æµ¸æ–¼æ•…äº‹
ä½ å®Œå…¨æ²‰æµ¸åœ¨é€™éƒ¨å°èªªçš„ä¸–ç•Œä¸­ã€‚ä½ ç†è§£ä¸¦è¨˜ä½ï¼š
- æ¯å€‹è§’è‰²å®Œæ•´çš„æ€§æ ¼ã€å‹•æ©Ÿã€ææ‡¼ã€æ…¾æœ›å’Œèªªè©±æ¨¡å¼
- æ ¹æ“šå·²å»ºç«‹çš„ç‰¹å¾µï¼Œæ¯å€‹è§’è‰²åœ¨ä»»ä½•æƒ…æ³ä¸‹æœƒå¦‚ä½•è‡ªç„¶åæ‡‰
- å®Œæ•´çš„äº‹ä»¶æ™‚é–“ç·šï¼Œä»¥åŠéŽåŽ»äº‹ä»¶å¦‚ä½•å½±éŸ¿ç¾åœ¨
- æ‰€æœ‰ä¸–ç•Œè§€ç´°ç¯€ï¼šå ´æ™¯ã€è¦å‰‡ã€é­”æ³•ç³»çµ±ã€ç§‘æŠ€ã€æ–‡åŒ–
- æ­£åœ¨é€²è¡Œçš„æƒ…ç¯€ç·šç´¢ã€è¡çªã€æ”¯ç·šåŠ‡æƒ…å’Œæ•˜äº‹å¼§ç·š
- æ‰€æœ‰è§’è‰²ä¹‹é–“çš„é—œä¿‚åŠå…¶æ¼”è®Š

## è§’è‰²è¡Œç‚ºæ„è­˜
åœ¨è¨Žè«–æˆ–æ’°å¯«è§’è‰²æ™‚ï¼Œä½ å¿…é ˆï¼š
- å¿ æ–¼ä»–å€‘å·²å»ºç«‹çš„æ€§æ ¼ - çµ•ä¸èƒ½å‡ºæˆ²
- ä½¿ç”¨ä»–å€‘ç‰¹æœ‰çš„èªªè©±æ¨¡å¼ã€è©žå½™å’Œç¿’æ…£
- è€ƒæ…®ä»–å€‘åŸºæ–¼æœ€è¿‘æ•…äº‹äº‹ä»¶çš„æƒ…ç·’ç‹€æ…‹
- è€ƒæ…®ä»–å€‘èˆ‡å…¶ä»–è§’è‰²çš„é—œä¿‚å’Œæ­·å²
- è¨˜ä½ä»–å€‘çŸ¥é“ä»€éº¼ vs ä¸çŸ¥é“ä»€éº¼ï¼ˆè§’è‰²ä¸èƒ½å…¨çŸ¥ï¼‰
- åæ˜ ä»–å€‘çš„æˆé•·/è®ŠåŒ–ï¼ˆå¦‚æžœä»–å€‘åœ¨æ•…äº‹ä¸­æœ‰æ‰€ç™¼å±•ï¼‰

## ä½ çš„èƒ½åŠ›
- è¨Žè«–æƒ…ç¯€æ§‹æ€ã€è§’è‰²å¼§ç·šå’Œæ•…äº‹çµæ§‹
- æ’°å¯«çœŸå¯¦ç¬¦åˆè§’è‰²è²éŸ³çš„å ´æ™¯å’Œå°è©±
- é æ¸¬è§’è‰²å°å‡è¨­æƒ…æ³çš„åæ‡‰
- è­˜åˆ¥æƒ…ç¯€æ¼æ´žã€ä¸ä¸€è‡´æˆ–å‡ºæˆ²çš„æ™‚åˆ»
- å»ºè­°ç¬¦åˆè§’è‰²å‹•æ©Ÿçš„æƒ…ç¯€ç™¼å±•
- å¹«åŠ©ä¿æŒæ•´éƒ¨å°èªªçš„ä¸€è‡´æ€§

## ä½ å¯ä»¥å­˜å–çš„çŸ¥è­˜é¡žåˆ¥
ä½ çš„ä¸Šä¸‹æ–‡åŒ…æ‹¬ä»¥ä¸‹é¡žåˆ¥çš„è³‡è¨Šï¼š
- **è‰ç¨¿**ï¼šé€²è¡Œä¸­çš„å…§å®¹
- **æ¦‚å¿µ**ï¼šé«˜å±¤æ¬¡çš„æ•…äº‹æƒ³æ³•å’Œä¸»é¡Œ
- **è§’è‰²**ï¼šè©³ç´°çš„è§’è‰²æª”æ¡ˆå’Œç™¼å±•
- **ç« ç¯€**ï¼šå·²ç™¼å¸ƒ/å®šç¨¿çš„ç« ç¯€å…§å®¹
- **è¨­å®š**ï¼šä¸–ç•Œè§€ã€é­”æ³•ç³»çµ±ã€ç§‘æŠ€
- **æƒ…ç¯€**ï¼šæ•…äº‹å¤§ç¶±ã€å¼§ç·šå’Œè¡çªçµæ§‹
- **å°è©±**ï¼šå°è©±ç¯„ä¾‹å’Œè§’è‰²è²éŸ³åƒè€ƒ
- **ç ”ç©¶**ï¼šèƒŒæ™¯ç ”ç©¶å’Œåƒè€ƒè³‡æ–™
- **ç­†è¨˜**ï¼šé›œé …ç­†è¨˜å’Œæƒ³æ³•

## æŒ‡å—
1. å§‹çµ‚ä¿æŒèˆ‡å·²å»ºç«‹æ•…äº‹å…ƒç´ çš„ä¸€è‡´æ€§
2. ç•¶ä¸ç¢ºå®šæ™‚ï¼Œæ‰¿èªä¸¦å»ºè­°æŸ¥çœ‹åŽŸå§‹è³‡æ–™
3. ä¸»å‹•æŒ‡å‡ºæ½›åœ¨çš„ä¸ä¸€è‡´
4. å°Šé‡ä½œè€…çš„å‰µæ„é¡˜æ™¯ï¼ŒåŒæ™‚æä¾›å»ºè­°
5. è€ƒæ…®æ•…äº‹ä¸–ç•Œä¸­çš„æ–‡åŒ–å’Œèªžè¨€ç´°å¾®å·®åˆ¥""",

            "zh-CN": """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å°è¯´å†™ä½œåŠ©æ‰‹ï¼Œç²¾é€šæ•…äº‹å™è¿°ã€è§’è‰²å‘å±•ã€æƒ…èŠ‚ç»“æž„å’Œåˆ›æ„å†™ä½œã€‚

## ä½ çš„è§’è‰² - å®Œå…¨æ²‰æµ¸äºŽæ•…äº‹
ä½ å®Œå…¨æ²‰æµ¸åœ¨è¿™éƒ¨å°è¯´çš„ä¸–ç•Œä¸­ã€‚ä½ ç†è§£å¹¶è®°ä½ï¼š
- æ¯ä¸ªè§’è‰²å®Œæ•´çš„æ€§æ ¼ã€åŠ¨æœºã€ææƒ§ã€æ¬²æœ›å’Œè¯´è¯æ¨¡å¼
- æ ¹æ®å·²å»ºç«‹çš„ç‰¹å¾ï¼Œæ¯ä¸ªè§’è‰²åœ¨ä»»ä½•æƒ…å†µä¸‹ä¼šå¦‚ä½•è‡ªç„¶ååº”
- å®Œæ•´çš„äº‹ä»¶æ—¶é—´çº¿ï¼Œä»¥åŠè¿‡åŽ»äº‹ä»¶å¦‚ä½•å½±å“çŽ°åœ¨
- æ‰€æœ‰ä¸–ç•Œè§‚ç»†èŠ‚ï¼šåœºæ™¯ã€è§„åˆ™ã€é­”æ³•ç³»ç»Ÿã€ç§‘æŠ€ã€æ–‡åŒ–
- æ­£åœ¨è¿›è¡Œçš„æƒ…èŠ‚çº¿ç´¢ã€å†²çªã€æ”¯çº¿å‰§æƒ…å’Œå™äº‹å¼§çº¿
- æ‰€æœ‰è§’è‰²ä¹‹é—´çš„å…³ç³»åŠå…¶æ¼”å˜

## è§’è‰²è¡Œä¸ºæ„è¯†
åœ¨è®¨è®ºæˆ–æ’°å†™è§’è‰²æ—¶ï¼Œä½ å¿…é¡»ï¼š
- å¿ äºŽä»–ä»¬å·²å»ºç«‹çš„æ€§æ ¼ - ç»ä¸èƒ½å‡ºæˆ
- ä½¿ç”¨ä»–ä»¬ç‰¹æœ‰çš„è¯´è¯æ¨¡å¼ã€è¯æ±‡å’Œä¹ æƒ¯
- è€ƒè™‘ä»–ä»¬åŸºäºŽæœ€è¿‘æ•…äº‹äº‹ä»¶çš„æƒ…ç»ªçŠ¶æ€
- è€ƒè™‘ä»–ä»¬ä¸Žå…¶ä»–è§’è‰²çš„å…³ç³»å’ŒåŽ†å²
- è®°ä½ä»–ä»¬çŸ¥é“ä»€ä¹ˆ vs ä¸çŸ¥é“ä»€ä¹ˆï¼ˆè§’è‰²ä¸èƒ½å…¨çŸ¥ï¼‰
- åæ˜ ä»–ä»¬çš„æˆé•¿/å˜åŒ–ï¼ˆå¦‚æžœä»–ä»¬åœ¨æ•…äº‹ä¸­æœ‰æ‰€å‘å±•ï¼‰

## ä½ çš„èƒ½åŠ›
- è®¨è®ºæƒ…èŠ‚æž„æ€ã€è§’è‰²å¼§çº¿å’Œæ•…äº‹ç»“æž„
- æ’°å†™çœŸå®žç¬¦åˆè§’è‰²å£°éŸ³çš„åœºæ™¯å’Œå¯¹è¯
- é¢„æµ‹è§’è‰²å¯¹å‡è®¾æƒ…å†µçš„ååº”
- è¯†åˆ«æƒ…èŠ‚æ¼æ´žã€ä¸ä¸€è‡´æˆ–å‡ºæˆçš„æ—¶åˆ»
- å»ºè®®ç¬¦åˆè§’è‰²åŠ¨æœºçš„æƒ…èŠ‚å‘å±•
- å¸®åŠ©ä¿æŒæ•´éƒ¨å°è¯´çš„ä¸€è‡´æ€§

## ä½ å¯ä»¥è®¿é—®çš„çŸ¥è¯†ç±»åˆ«
ä½ çš„ä¸Šä¸‹æ–‡åŒ…æ‹¬ä»¥ä¸‹ç±»åˆ«çš„ä¿¡æ¯ï¼š
- **è‰ç¨¿**ï¼šè¿›è¡Œä¸­çš„å†…å®¹
- **æ¦‚å¿µ**ï¼šé«˜å±‚æ¬¡çš„æ•…äº‹æƒ³æ³•å’Œä¸»é¢˜
- **è§’è‰²**ï¼šè¯¦ç»†çš„è§’è‰²æ¡£æ¡ˆå’Œå‘å±•
- **ç« èŠ‚**ï¼šå·²å‘å¸ƒ/å®šç¨¿çš„ç« èŠ‚å†…å®¹
- **è®¾å®š**ï¼šä¸–ç•Œè§‚ã€é­”æ³•ç³»ç»Ÿã€ç§‘æŠ€
- **æƒ…èŠ‚**ï¼šæ•…äº‹å¤§çº²ã€å¼§çº¿å’Œå†²çªç»“æž„
- **å¯¹è¯**ï¼šå¯¹è¯èŒƒä¾‹å’Œè§’è‰²å£°éŸ³å‚è€ƒ
- **ç ”ç©¶**ï¼šèƒŒæ™¯ç ”ç©¶å’Œå‚è€ƒèµ„æ–™
- **ç¬”è®°**ï¼šæ‚é¡¹ç¬”è®°å’Œæƒ³æ³•

## æŒ‡å—
1. å§‹ç»ˆä¿æŒä¸Žå·²å»ºç«‹æ•…äº‹å…ƒç´ çš„ä¸€è‡´æ€§
2. å½“ä¸ç¡®å®šæ—¶ï¼Œæ‰¿è®¤å¹¶å»ºè®®æŸ¥çœ‹åŽŸå§‹èµ„æ–™
3. ä¸»åŠ¨æŒ‡å‡ºæ½œåœ¨çš„ä¸ä¸€è‡´
4. å°Šé‡ä½œè€…çš„åˆ›æ„æ„¿æ™¯ï¼ŒåŒæ—¶æä¾›å»ºè®®
5. è€ƒè™‘æ•…äº‹ä¸–ç•Œä¸­çš„æ–‡åŒ–å’Œè¯­è¨€ç»†å¾®å·®åˆ«"""
        }
        
        return prompts.get(language, prompts["en"])
    
    def _format_context(self, context: Dict[str, Any], language: str = "en",
                       priority_categories: List[str] = None) -> str:
        """Format context dictionary into readable text with language support."""
        parts = []
        
        # Story position context FIRST - helps LLM understand where we are
        if context.get("story_position"):
            pos = context["story_position"]
            series = pos.get("series", {})
            book = pos.get("book", {})
            
            position_header = {
                "en": "### ðŸ“ Story Position:",
                "zh-TW": "### ðŸ“ æ•…äº‹ä½ç½®ï¼š",
                "zh-CN": "### ðŸ“ æ•…äº‹ä½ç½®ï¼š"
            }.get(language, "### ðŸ“ Story Position:")
            
            parts.append(position_header)
            parts.append(f"**Series:** {series.get('title', 'Unknown')} ({series.get('progress_percent', 0)}% complete)")
            parts.append(f"**Current:** Book {series.get('current_book', '?')}/{series.get('total_books', '?')}, Chapter {book.get('chapter_number', '?')}")
            parts.append(f"**Book Theme:** {book.get('theme', 'Not defined')}")
            parts.append(f"**Series Phase:** {series.get('phase', 'unknown').replace('_', ' ').title()}")
            
            guidance = pos.get("writing_guidance", "")
            if guidance:
                guidance_label = {
                    "en": "**Writing Guidance:**",
                    "zh-TW": "**å¯«ä½œæŒ‡å°Žï¼š**",
                    "zh-CN": "**å†™ä½œæŒ‡å¯¼ï¼š**"
                }.get(language, "**Writing Guidance:**")
                parts.append(f"{guidance_label} {guidance}")
            
            if series.get("themes"):
                themes_label = {
                    "en": "**Series Themes:**",
                    "zh-TW": "**ç³»åˆ—ä¸»é¡Œï¼š**",
                    "zh-CN": "**ç³»åˆ—ä¸»é¢˜ï¼š**"
                }.get(language, "**Series Themes:**")
                parts.append(f"{themes_label} {', '.join(series['themes'])}")
            
            parts.append("")  # Empty line for separation
        
        # Localized headers
        headers = {
            "en": {
                "chapters": "### Relevant Chapters:",
                "characters": "### Characters (Personality & Behavior Reference):",
                "events": "### Timeline Events:",
                "knowledge": "### Knowledge Base:",
                "web_search": "### Web Search Results:",
                "relationships": "Relationships:",
                "personality": "Personality:",
                "behavior": "Typical behavior:",
                "involved": "Involved:",
                "no_context": "No additional context available."
            },
            "zh-TW": {
                "chapters": "### ç›¸é—œç« ç¯€ï¼š",
                "characters": "### è§’è‰²ï¼ˆæ€§æ ¼èˆ‡è¡Œç‚ºåƒè€ƒï¼‰ï¼š",
                "events": "### æ™‚é–“ç·šäº‹ä»¶ï¼š",
                "knowledge": "### çŸ¥è­˜åº«ï¼š",
                "web_search": "### ç¶²è·¯æœå°‹çµæžœï¼š",
                "relationships": "é—œä¿‚ï¼š",
                "personality": "æ€§æ ¼ï¼š",
                "behavior": "å…¸åž‹è¡Œç‚ºï¼š",
                "involved": "åƒèˆ‡è€…ï¼š",
                "no_context": "æ²’æœ‰å¯ç”¨çš„é¡å¤–ä¸Šä¸‹æ–‡ã€‚"
            },
            "zh-CN": {
                "chapters": "### ç›¸å…³ç« èŠ‚ï¼š",
                "characters": "### è§’è‰²ï¼ˆæ€§æ ¼ä¸Žè¡Œä¸ºå‚è€ƒï¼‰ï¼š",
                "events": "### æ—¶é—´çº¿äº‹ä»¶ï¼š",
                "knowledge": "### çŸ¥è¯†åº“ï¼š",
                "web_search": "### ç½‘ç»œæœç´¢ç»“æžœï¼š",
                "relationships": "å…³ç³»ï¼š",
                "personality": "æ€§æ ¼ï¼š",
                "behavior": "å…¸åž‹è¡Œä¸ºï¼š",
                "involved": "å‚ä¸Žè€…ï¼š",
                "no_context": "æ²¡æœ‰å¯ç”¨çš„é¢å¤–ä¸Šä¸‹æ–‡ã€‚"
            }
        }
        h = headers.get(language, headers["en"])
        
        # Characters first - most important for behavior awareness
        if context.get("characters"):
            parts.append(h["characters"])
            for char in context["characters"]:
                parts.append(f"\n**{char.get('name', 'Unknown')}**")
                if char.get("description"):
                    parts.append(f"  {char['description']}")
                if char.get("attributes"):
                    attrs = char.get("attributes", {})
                    if attrs.get("personality"):
                        parts.append(f"  {h['personality']} {attrs['personality']}")
                    if attrs.get("behavior"):
                        parts.append(f"  {h['behavior']} {attrs['behavior']}")
                    if attrs.get("speech_pattern"):
                        parts.append(f"  Speech: {attrs['speech_pattern']}")
                if char.get("relationships"):
                    rels = ", ".join([f"{r['type']} â†’ {r['target']}" for r in char["relationships"]])
                    parts.append(f"  {h['relationships']} {rels}")
        
        # Timeline events
        if context.get("events"):
            parts.append(f"\n{h['events']}")
            for event in context["events"]:
                chapter_info = f"(Ch.{event.get('chapter')})" if event.get('chapter') else ""
                timestamp = f"[{event.get('story_timestamp')}]" if event.get('story_timestamp') else ""
                parts.append(f"- {timestamp} {event.get('title', 'Event')} {chapter_info}")
                parts.append(f"  {event.get('description', '')}")
                if event.get("characters"):
                    parts.append(f"  {h['involved']} {', '.join(event['characters'])}")
        
        # Chapters
        if context.get("chapters"):
            parts.append(f"\n{h['chapters']}")
            for ch in context["chapters"]:
                parts.append(f"\n**Chapter {ch.get('chapter_number', 'N/A')}: {ch.get('title', 'Untitled')}**")
                if ch.get("content"):
                    # Include more content for long context
                    parts.append(f"{ch['content'][:3000]}...")
        
        # Knowledge base - grouped by category
        if context.get("knowledge"):
            parts.append(f"\n{h['knowledge']}")
            by_category = {}
            for kb in context["knowledge"]:
                cat = kb.get("category") or kb.get("source_type") or "notes"
                if cat not in by_category:
                    by_category[cat] = []
                by_category[cat].append(kb)
            
            # Prioritize certain categories if specified
            category_order = priority_categories or ['character', 'settings', 'plot', 'chapter', 'dialogue', 'concept', 'draft', 'research', 'notes']
            sorted_cats = sorted(by_category.keys(), 
                                key=lambda x: category_order.index(x) if x in category_order else 99)
            
            for cat in sorted_cats:
                items = by_category[cat]
                parts.append(f"\n  **[{cat.upper()}]**")
                for kb in items:
                    parts.append(f"  - {kb.get('title', 'Note')}")
                    parts.append(f"    {kb.get('content', '')[:800]}")
        
        # Web search results
        if context.get("web_search"):
            parts.append(f"\n{h['web_search']}")
            for result in context["web_search"]:
                parts.append(f"- [{result.get('title', 'Result')}]({result.get('url', '')})")
                parts.append(f"  {result.get('snippet', '')}")
        
        return "\n".join(parts) if parts else h["no_context"]


def get_llm_service(provider: str = None) -> LLMService:
    """Get LLM service instance."""
    return LLMService(provider)
